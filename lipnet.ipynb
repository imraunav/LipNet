{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LipNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './weights/LipNet_unseen_loss_0.44562849402427673_wer_0.1332580699113564_cer_0.06796452465503355.pt'\n",
    "lipnet = LipNet()\n",
    "lipnet.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'conv3.weight', 'conv3.bias', 'gru1.weight_ih_l0', 'gru1.weight_hh_l0', 'gru1.bias_ih_l0', 'gru1.bias_hh_l0', 'gru1.weight_ih_l0_reverse', 'gru1.weight_hh_l0_reverse', 'gru1.bias_ih_l0_reverse', 'gru1.bias_hh_l0_reverse', 'gru2.weight_ih_l0', 'gru2.weight_hh_l0', 'gru2.bias_ih_l0', 'gru2.bias_hh_l0', 'gru2.weight_ih_l0_reverse', 'gru2.weight_hh_l0_reverse', 'gru2.bias_ih_l0_reverse', 'gru2.bias_hh_l0_reverse', 'FC.weight', 'FC.bias'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(PATH, map_location=torch.device('cpu')).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os \n",
    "from preprocessing import TokenConv, get_frames_pkl, load_align, HorizontalFlip, padding\n",
    "import cv2\n",
    "\n",
    "class LipDataset(Dataset):\n",
    "    def __init__(self, dataset_path, vid_pad=75, align_pad=40, phase=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        self.align_path = os.path.join(dataset_path, phase, \"alignments\")\n",
    "        # self.vid_path = os.path.join(dataset_path, phase, \"videos\")\n",
    "        self.frames_path = os.path.join(dataset_path, phase, \"frames\")\n",
    "        self.vid_pad = vid_pad\n",
    "        self.align_pad = align_pad\n",
    "        self.phase = phase\n",
    "        self.ctccoder = TokenConv()\n",
    "\n",
    "        self.data = []\n",
    "        for path, subdirs, files in os.walk(self.frames_path):\n",
    "            if len(subdirs) != 0:  # if not in subdir, don't do anything\n",
    "                continue\n",
    "\n",
    "            spk = path.split(os.path.sep)[-1]  # only speaker name from path\n",
    "            # print(\"Speaker: \", spk)\n",
    "\n",
    "            for file in files:\n",
    "                # if \".mpg\" not in file:  # skip non-video files\n",
    "                #     continue\n",
    "                if \".pkl\" not in file:  # skip non-pickle files\n",
    "                    continue\n",
    "                # print((spk, file.split(\".\")[0]))\n",
    "\n",
    "                fname = file.split(\".\")[0]  # only name of the file without extention\n",
    "                align_dir = os.path.join(self.align_path, spk, fname + \".align\")\n",
    "                if os.path.exists(align_dir):  # only add when the alignment also exists\n",
    "                    self.data.append((spk, fname))  # speaker-name and name of the file\n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        speaker, fname = self.data[index]\n",
    "        frames_path = os.path.join(self.frames_path, speaker, fname + \".pkl\")\n",
    "        align_path = os.path.join(self.align_path, speaker, fname + \".align\")\n",
    "\n",
    "        vid = get_frames_pkl(frames_path)\n",
    "        for i, v in enumerate(vid):\n",
    "            vid[i] = cv2.resize(v, (128, 64))\n",
    "        align = load_align(align_path)\n",
    "        align = self.ctccoder.encode(align)\n",
    "\n",
    "        if self.phase == \"train\":\n",
    "            vid = HorizontalFlip(vid)\n",
    "\n",
    "        vid_len = len(vid)\n",
    "        align_len = len(align)\n",
    "        vid = padding(vid, self.vid_pad)\n",
    "        align = padding(align, self.align_pad)\n",
    "\n",
    "        return (\n",
    "            torch.Tensor(vid)/255.0, # normalization\n",
    "            torch.Tensor(align),\n",
    "            vid_len,\n",
    "            align_len,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "dataset = LipDataset(\"./dataset\", phase='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s1', 'pbac2p'),\n",
       " ('s1', 'pgwe6n'),\n",
       " ('s1', 'prap7a'),\n",
       " ('s1', 'bwwn6n'),\n",
       " ('s1', 'praj1s'),\n",
       " ('s1', 'lbakzn'),\n",
       " ('s1', 'prbd1s'),\n",
       " ('s1', 'pgbk9a')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from preprocessing import TokenConv\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True:  place blue at c two please\n",
      "Pred:  bin blue in d twro again\n",
      "----------\n",
      "True:  place gren with e six now\n",
      "Pred:  bin brue in j six soon\n",
      "----------\n",
      "True:  place red at sp p seven again\n",
      "Pred:  bin red with d nine now\n",
      "----------\n",
      "True:  bin white sp with n six now\n",
      "Pred:  bin white with y six again\n",
      "----------\n",
      "True:  place red at j one son\n",
      "Pred:  bin white by d two soon\n",
      "----------\n",
      "True:  lay blue at k zero now\n",
      "Pred:  place re at t twro soon\n",
      "----------\n",
      "True:  place red by d one son\n",
      "Pred:  bin red by b six soon\n",
      "----------\n",
      "True:  place gren by k nine again\n",
      "Pred:  bin brue by p nine now\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "ctcdecoder = TokenConv()\n",
    "lipnet.eval()\n",
    "for vid, align, vid_len, align_len in loader:\n",
    "    y = lipnet(vid).log_softmax(-1)\n",
    "    y = torch.argmax(y, dim=-1)\n",
    "    for tru, pre in zip(align.tolist(), y.tolist()):\n",
    "        true_txt = ctcdecoder.ctc_decode(tru)\n",
    "        pred_txt = ctcdecoder.ctc_decode(pre)\n",
    "        print(\"True: \", true_txt)\n",
    "        print(\"Pred: \", pred_txt)\n",
    "        print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
